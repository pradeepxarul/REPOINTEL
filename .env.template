# ============= OLLAMA CONFIGURATION (Local LLM - Zero Cost) =============

# Enable Ollama for local LLM inference (set to true to use Ollama instead of cloud APIs)
USE_OLLAMA=true

# Ollama server URL
# For local deployment: http://localhost:11434
# For remote GPU server: http://your-gpu-server-ip:11434
OLLAMA_URL=http://localhost:11434

# Ollama model to use
# Recommended: llama3.1:8b (6GB VRAM, fast, good quality)
# Alternative: llama3.1:8b-q4_K_M (4-bit quantized, 4GB VRAM)
# Premium: llama3.1:70b (40GB+ VRAM, best quality)
OLLAMA_MODEL=llama3.1:8b


# ============= CLOUD LLM FALLBACK (Optional but Recommended) =============

# GROQ API (Primary fallback - Fast & Cheap)
# Get key from: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here
LLM_MODEL=llama-3.3-70b-versatile

# OpenAI API (Alternative fallback)
# OPENAI_API_KEY=your_openai_key_here

# Google Gemini API (Alternative fallback)
# GOOGLE_API_KEY=your_google_key_here

# LLM Provider (if not using GROQ)
LLM_PROVIDER=groq

# LLM Settings
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=4096


# ============= GITHUB APP CREDENTIALS (REQUIRED) =============

# GitHub App ID (from your GitHub App settings)
GITHUB_APP_ID=your_app_id_here

# GitHub Installation ID (from your GitHub App installation)
GITHUB_INSTALLATION_ID=your_installation_id_here

# GitHub Private Key (from your GitHub App)
# Replace newlines with \n in the key
GITHUB_PRIVATE_KEY="-----BEGIN RSA PRIVATE KEY-----\nYOUR_PRIVATE_KEY_HERE\n-----END RSA PRIVATE KEY-----"


# ============= SERVICE CONFIGURATION =============

# Maximum repositories to analyze per user
MAX_REPOS_PER_USER=15

# Cache TTL in seconds (24 hours = 86400)
CACHE_TTL_SECONDS=86400

# API timeout in seconds
API_TIMEOUT_SECONDS=10

# Logging level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Server port
PORT=8000

# Environment (development, production, test)
ENVIRONMENT=production
