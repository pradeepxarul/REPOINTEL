# ğŸ”„ Complete System Workflow

## System Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT REQUEST                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  FASTAPI APPLICATION                         â”‚
â”‚                     (main.py)                                â”‚
â”‚  - CORS middleware                                           â”‚
â”‚  - Error handling                                            â”‚
â”‚  - Swagger docs                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API ROUTES                                â”‚
â”‚                  (api/routes.py)                             â”‚
â”‚  - POST /api/v1/analyze                                      â”‚
â”‚  - POST /api/v1/reports/generate                            â”‚
â”‚  - GET /health                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                             â”‚
        â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYZE FLOW      â”‚    â”‚  REPORT FLOW       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â”‚                         â”‚
    [See Below]               [See Below]
```

---

## 1ï¸âƒ£ Analyze Profile Flow

### Request
```http
POST /api/v1/analyze
Content-Type: application/json

{
  "github_input": "username"
}
```

### Processing Steps

```
1. VALIDATION
   â””â”€> utils/validators.py
       - Normalize input (URL â†’ username)
       - Validate format

2. CACHE CHECK
   â””â”€> services/cache_service.py
       - Check if cached (<24h)
       - If HIT â†’ Return cached data
       - If MISS â†’ Continue to GitHub

3. GITHUB API CALLS (Parallel)
   â””â”€> services/github_service.py
       
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  User Profile                   â”‚
       â”‚  GET /users/{username}          â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Repositories List              â”‚
       â”‚  GET /users/{username}/repos    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  FOR EACH REPO (Parallel):      â”‚
       â”‚                                 â”‚
       â”‚  1. Languages                   â”‚
       â”‚     GET /repos/{}/languages     â”‚
       â”‚                                 â”‚
       â”‚  2. README                      â”‚
       â”‚     GET /repos/{}/readme        â”‚
       â”‚                                 â”‚
       â”‚  3. Repository Tree             â”‚
       â”‚     GET /repos/{}/git/trees/{}  â”‚
       â”‚                                 â”‚
       â”‚  4. ALL Markdown Files          â”‚
       â”‚     - Filter *.md files         â”‚
       â”‚     - GET /repos/{}/contents/{} â”‚
       â”‚     - Extract FULL content      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. DATA PROCESSING
   - Calculate language percentages
   - Count markdown files
   - Compute activity metrics (Optimization: Uses `pushed_at` from repo data, saving N API calls)
   - Format response structure

5. CACHE STORAGE
   â””â”€> services/cache_service.py
       - Save to cache/
       - Set 24h TTL

6. JSON STORAGE
   â””â”€> services/storage_service.py
       - Save to db/{username}.json
       - Include ALL data
       - Complete markdown files

7. RESPONSE
   - Return formatted JSON
   - Include performance metrics
```

### Response Structure
```json
{
  "status": "success",
  "user": { /* profile */ },
  "repositories": [
    {
      "name": "repo-name",
      "languages": { /* percentages */ },
      "readme": {
        "content": "... full content ...",
        "length_chars": 1234
      },
      "markdown_files": [
        {
          "filename": "CONTRIBUTING.md",
          "path": "docs/CONTRIBUTING.md",
          "content": "... COMPLETE ...  ",
          "length_chars": 2500
        }
      ]
    }
  ],
  "performance": {
    "total_latency_ms": 1850
  }
}
```

---

## 2ï¸âƒ£ Generate Report Flow

### Request
```http
POST /api/v1/reports/generate
Content-Type: application/json

{
  "username": "developer",
  "report_type": "full",
  "use_stored": true
}
```

### Processing Steps

```
1. DATA RETRIEVAL
   â”œâ”€> Check use_stored flag
   â”‚
   â”œâ”€> IF use_stored = true:
   â”‚   â””â”€> services/storage_service.py
   â”‚       - Load db/{username}.json
   â”‚       - If found â†’ Use stored data
   â”‚       - If not found â†’ Fetch from GitHub
   â”‚
   â””â”€> IF use_stored = false:
       â””â”€> Call analyze endpoint
           - Fresh GitHub fetch

2. METRICS CALCULATION
   â””â”€> services/llm_service.py
       
       Calculate:
       - Total repos, stars, forks
       - Language distribution
       - Documentation coverage
       - Markdown file count
       - Activity patterns
       - Account age
       - Quality indicators

3. LLM PROVIDER SELECTION
   â””â”€> Check API keys in order:
       
       1. GROQ_API_KEY exists?
          â””â”€> Use GROQ (Primary)
       
       2. OPENAI_API_KEY exists?
          â””â”€> Use OpenAI
       
       3. GOOGLE_API_KEY exists?
          â””â”€> Use Gemini
       
       4. No keys?
          â””â”€> Template mode

4. PROMPT BUILDING
   - Comprehensive developer profile
   - Quantitative metrics
   - Top repositories with details
   - Markdown documentation evidence
   - Code quality indicators

5. LLM API CALL
   
   â”Œâ”€ GROQ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Model: llama-3.3-70b-versatile   â”‚
   â”‚  Temp: 0.1                        â”‚
   â”‚  Max Tokens: 2048                 â”‚
   â”‚  Speed: ~2-3 seconds              â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   
   OR
   
   â”Œâ”€ OpenAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Model: gpt-4o                    â”‚
   â”‚  Temp: configured                 â”‚
   â”‚  Max Tokens: configured           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

6. RESPONSE PARSING
   - Extract JSON from LLM response
   - Validate structure
   - Add candidate metadata
   - Include generation timestamp

7. STRUCTURED REPORT
   {
     "candidate": { /* info */ },
     "executive_summary": "...",
     "technical_assessment": {
       "overall_score": 7.5,
       "frameworks_detected": [...],
       "specializations": [...]
     },
     "code_quality": { /* scores */ },
     "project_analysis": { /* projects */ },
     "activity_profile": { /* patterns */ },
     "strengths": [...],
     "areas_for_improvement": [...],
     "hiring_recommendation": {
       "overall_score": 7.5,
       "suitable_roles": [...],
       "next_steps": [...]
     }
   }
```

---

## ğŸ”„ Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP Request
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API Routes         â”‚
â”‚  - Validate input    â”‚
â”‚  - Route to handler  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Services Layer     â”‚
â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GitHub Service â”‚  â”‚
â”‚  â”‚  - Fetch data  â”‚  â”‚
â”‚  â”‚  - Extract MD  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Storage Serviceâ”‚  â”‚
â”‚  â”‚  - Save JSON   â”‚  â”‚
â”‚  â”‚  - Load data   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LLM Service   â”‚  â”‚
â”‚  â”‚ - GROQ/GPT-4   â”‚  â”‚
â”‚  â”‚ - Generate     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Cache Service  â”‚  â”‚
â”‚  â”‚  - File cache  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Storage Layer      â”‚
â”‚  - db/ (JSON files)  â”‚
â”‚  - cache/ (temp)     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client Response    â”‚
â”‚  - JSON formatted    â”‚
â”‚  - Complete data     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Processes

### Markdown Extraction Process
```
1. Get repository tree
   GET /repos/{owner}/{repo}/git/trees/{branch}?recursive=1

2. Filter markdown files
   - Find all files ending with .md
   - Case-insensitive matching
   - Exclude README.md (fetched separately)

3. Fetch content in parallel
   - For each .md file:
     GET /repos/{owner}/{repo}/contents/{path}
   - Decode base64 content
   - Extract FULL text (no truncation)

4. Store results
   {
     "filename": "CONTRIBUTING.md",
     "path": "docs/CONTRIBUTING.md",
     "content": "... complete ...",
     "length_chars": 2500
   }
```

### LLM Provider Selection
```python
# Priority order:
if GROQ_API_KEY:
    use_provider("groq")
    use_model("llama-3.3-70b-versatile")
elif OPENAI_API_KEY:
    use_provider("openai")
    use_model("gpt-4o")
elif GOOGLE_API_KEY:
    use_provider("gemini")
    use_model("gemini-pro")
else:
    use_template_mode()
```

---

## âš¡ Performance Optimizations

1. **Parallel API Calls** - All repo data fetched concurrently
2. **Smart Caching** - 24-hour TTL reduces GitHub API load
3. **JSON Storage** - Fast file-based persistence
4. **Batch Processing** - Multiple markdown files fetched in parallel
5. **Connection Pooling** - Reuse HTTP connections

---

## ğŸ”§ Configuration Changes

### Changing LLM Model

**Update `.env`:**
```env
# For GROQ
GROQ_API_KEY=gsk_your_key
LLM_MODEL=llama-3.3-70b-versatile  # or llama-3.1-8b-instant

# For OpenAI
GROQ_API_KEY=  # Remove/empty
OPENAI_API_KEY=sk-your_key
LLM_MODEL=gpt-4o  # or gpt-4-turbo

# Adjust parameters
LLM_TEMPERATURE=0.1  # 0.0-1.0
LLM_MAX_TOKENS=2048  # Response length
```

**Restart server** - Changes apply immediately!

---

## ğŸ“Š System Metrics

- **API Calls per Analysis**: 32-50 (depending on repos)
- **Cache Hit Rate**: ~60-70% (production)
- **Average Response Time**: 1.5s (uncached), 20ms (cached)
- **LLM Report Time**: 2-5s (GROQ), 5-10s (OpenAI)
- **Storage Size**: ~30-50KB per user profile (JSON)

---

## ğŸš€ Deployment Workflow

```
1. Setup Environment
   - Install Python 3.11+
   - Create venv
   - pip install -r requirements.txt

2. Configure
   - Copy .env.example â†’ .env
   - Add GitHub App credentials
   - Add GROQ API key

3. Test Locally
   - python src/main.py
   - Test at http://localhost:8000/docs

4. Deploy
   - Docker container OR
   - PM2 process manager OR
   - Cloud platform (AWS, Azure, GCP)

5. Monitor
   - Check logs
   - Monitor API calls
   - Track performance
```

---

This workflow ensures:
- âœ… Complete data extraction
- âœ… Fast response times
- âœ… Reliable AI reports
- âœ… Production-grade reliability
